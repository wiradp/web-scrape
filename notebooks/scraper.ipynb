{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4825830a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Production-ready scraper module.\n",
    "Contains a polite scraping function with retries, user-agent rotation, and saving to database.\n",
    "\"\"\"\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time, random, csv, hashlib, logging, re\n",
    "from typing import List, Dict, Optional\n",
    "from urllib.parse import urljoin\n",
    "import sqlite3\n",
    "from datetime import datetime\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "DEFAULT_HEADERS = {\n",
    "    'User-Agent': 'web-scrape-bot/1.0 (+https://wiradp.github.io)'\n",
    "}\n",
    "\n",
    "def fetch_url(url: str, headers: Optional[dict]=None, timeout: int=10, retries: int=3) -> Optional[str]:\n",
    "    headers = headers or DEFAULT_HEADERS\n",
    "    for attempt in range(1, retries+1):\n",
    "        try:\n",
    "            resp = requests.get(url, headers=headers, timeout=timeout)\n",
    "            resp.raise_for_status()\n",
    "            return resp.text\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"fetch_url attempt {attempt} failed for {url}: {e}\")\n",
    "            time.sleep(min(2**attempt, 10) + random.random())\n",
    "    return None\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Membersihkan teks dari whitespace characters (\\r, \\n, \\t) dan multiple spaces.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return text\n",
    "    \n",
    "    # Replace \\r, \\n, \\t dengan space\n",
    "    cleaned = re.sub(r'[\\r\\n\\t]+', ' ', text)\n",
    "    # Replace multiple spaces dengan single space\n",
    "    cleaned = re.sub(r'\\s+', ' ', cleaned)\n",
    "    # Trim leading and trailing spaces\n",
    "    return cleaned.strip()\n",
    "\n",
    "def parse_listing_page(html: str, base_url: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Mengekstrak informasi produk dari elemen <table> dengan pembersihan teks.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    items = []\n",
    "\n",
    "    # Menargetkan tabel utama di halaman\n",
    "    table = soup.find('table')\n",
    "    if not table:\n",
    "        print(\"Elemen <table> tidak ditemukan.\")\n",
    "        return []\n",
    "\n",
    "    # Mengambil semua baris (tr) di dalam tabel\n",
    "    rows = table.find_all('tr')\n",
    "    \n",
    "    # Loop melalui setiap baris, lewati baris header jika perlu\n",
    "    for row in rows[1:]: # Mulai dari indeks 1 untuk melewati header\n",
    "        try:\n",
    "            cols = row.find_all('td')\n",
    "            # Memastikan baris memiliki setidaknya 2 kolom (nama dan harga)\n",
    "            if len(cols) >= 2:\n",
    "                # Kolom pertama (indeks 0) adalah nama produk - DIBERSIHKAN\n",
    "                raw_name = cols[0].get_text()\n",
    "                name = clean_text(raw_name)\n",
    "                \n",
    "                # Kolom kedua (indeks 1) adalah harga - DIBERSIHKAN\n",
    "                raw_price = cols[1].get_text()\n",
    "                price_str = clean_text(raw_price)\n",
    "                \n",
    "                # Membersihkan harga dari 'Rp', '.', dan spasi\n",
    "                price_cleaned = int(re.sub(r'[^\\d]', '', price_str))\n",
    "                \n",
    "                # Hanya tambahkan jika nama dan harga valid\n",
    "                if name and price_cleaned > 0:\n",
    "                    items.append({\n",
    "                        'product_name': name,\n",
    "                        'price_raw': price_cleaned\n",
    "                    })\n",
    "        except (ValueError, IndexError) as e:\n",
    "            # Lewati baris yang mungkin kosong atau formatnya salah\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"Error memproses baris: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return items\n",
    "    \n",
    "def save_to_db(rows: List[Dict], db_path: str = 'data/database/laptop_data_raw.db'):\n",
    "    \"\"\"\n",
    "    Menyimpan data yang dikumpulkan ke dalam basis data SQLite pada tabel `products_raw`.\n",
    "    Membuat tabel jika belum ada.\n",
    "    Menggunakan executemany untuk penyisipan massal yang efisien.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    # Buat direktori jika belum ada\n",
    "    os.makedirs(os.path.dirname(db_path), exist_ok=True)\n",
    "\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Buat table jika belum ada\n",
    "    cursor.execute(\"\"\"\n",
    "                   CREATE TABLE IF NOT EXISTS product_raw (\n",
    "                   id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                   product_name TEXT NOT NULL,\n",
    "                   price_raw INTEGER NOT NULL,\n",
    "                   scraped_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "                   );\n",
    "    \"\"\")\n",
    "\n",
    "    # Persiapkan data untuk bulk insert\n",
    "    records = [(item['product_name'], item['price_raw'], datetime.now()) for item in rows]\n",
    "\n",
    "    # Gunakan executemary untuk insert batch\n",
    "    cursor.executemany(\"\"\"\n",
    "                       INSERT INTO product_raw (product_name, price_raw, scraped_at)\n",
    "                       VALUES (?, ?, ?);\n",
    "    \"\"\", records)\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    print(f'Done, {len(rows)} items save to database: {db_path}')    \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    url = 'https://viraindo.com/notebook.html'\n",
    "    html = fetch_url(url)\n",
    "    if html:\n",
    "        items = parse_listing_page(html, url)\n",
    "        # Ganti save_to_csv dengan save_to_db\n",
    "        save_to_db(items, 'data/database/laptop_data_raw.db')\n",
    "        # save_to_csv(items, 'data/raw/notebooks_viraindo_scraped.csv')\n",
    "        print(f\"Scraping done. Total {len(items)} items.\")\n",
    "    else:\n",
    "        print('Scraping failed')\n",
    "            \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (web-scrape)",
   "language": "python",
   "name": "web-scrape-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
